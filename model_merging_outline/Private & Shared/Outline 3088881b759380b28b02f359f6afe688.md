# Outline

1.  **Hook**
    1. Here's a scenario that's becoming increasingly common at frontier labs. Three teams, working from the same base model — one focused on safety and alignment, another on code reasoning, a third on RLVR across diverse reasoning environments. Each team produces a strong specialist. Now leadership wants a single model that combines all three capabilities.
    The obvious path is to retrain from scratch with all objectives jointly which is expensive, logistically painful (you need everyone's data and pipelines), and risks regression on capabilities that already work.
    What if you could just take the average the weights of all three models? Would that work?
    <ask the audience>
    Turns out, it actually does work.
    2. Show actual benchmark numbers or a leaderboard screenshot to make it tangible
    3. Of course, it's not that simple. There are some caveats.
2.  **The Geometry of the Loss Landscape**
    1. **The loss landscape.**
        [Animation: a rugged, chaotic 3D loss surface. Two dots — Model A and Model B — sitting in two completely different valleys, separated by massive ridges. Draw the straight line between them. It goes right over a mountain. Show the loss spiking.]
        "If we average their weights, we land here — on top of a ridge. The merged model is garbage."
    2. **What is a "mode"?**
        A mode is a valley — a region of parameter space that a training run converges to. Different random initializations, different data orders, different hyperparameters can all land you in different modes. Each mode is a valid solution, but they can be far apart.
    3. **Curved connectivity (Garipov et al. 2018, Draxler et al. 2018):**
        [Animation: two models in different valleys. Show a curving path that winds around the ridges, staying in low-loss territory.]
        Two independent groups showed these curved paths always exist. Models *are* connected — but by curves, not straight lines. Weight averaging walks a straight line. Curved connectivity tells us something deep about the landscape, but it doesn't explain why merging works.
    4.  **From curves to straight lines: when does LMC hold? (Frankle et al. 2020)**
        1. **The forking experiment:** Train a model for k steps from random initialization. Fork it into two identical copies. Continue training each copy to convergence with *different* SGD noise (different data order, different augmentation). Now check: is the straight-line path between the two final models low-loss?
        [Animation: a single training trajectory that splits into two branches at step k. Both branches converge to points in the same valley.]
        2. **The key finding: there is a critical k.** Before k, the two copies drift into different basins and LMC fails. After k, they stay in the same basin and LMC holds. This is a sharp phase transition, not a gradual one.
        3. **Scale dependence:** On MNIST, stability happens at initialization (k=0). On ImageNet with ResNet-50 and Inception-v3, stability emerges early in training, well before convergence.
        4. **The punchline for model merging:** Fine-tuning from a pretrained base is exactly the forking experiment. The "shared training" is the entire pretraining run (hundreds of billions of tokens). The stability threshold is crossed very early in training. So fine-tuned models from the same base have vastly more shared optimization than LMC requires.
        [Show a plot: interpolation coefficient α ∈ [0,1] on x-axis, loss on y-axis. Flat curve when LMC holds, barrier when it doesn't.]
    5. **When LMC breaks: permutation symmetries (Entezari et al. 2021)**
        1. LMC does *not* hold between independently trained models. Straight-line interpolation hits a massive barrier. This is the default case.
        2. **Why?** Neural networks have a built-in symmetry: you can swap any two neurons in a hidden layer (along with their incoming and outgoing weights) and get a functionally identical network. Each layer with N neurons has N! equivalent arrangements.
        [Animation: two hidden layers with color-coded neurons. Same features learned, but in different positions. Averaging them naively mixes unrelated neurons.]
        3. **Entezari's conjecture:** The barriers between independently trained models are not from fundamentally different solutions. They are from permutation symmetries misaligning equivalent neurons. If you find the right permutation to align them, the barrier should vanish.
        4. **Git Re-Basin (Ainsworth et al., ICLR 2023):** Three algorithms to find the aligning permutation. Achieved zero-barrier LMC between independently trained ResNets on CIFAR-10/100. What looked like a complex non-convex landscape turns out to be nearly convex once you account for permutation symmetry.
        5. **REPAIR (Jordan et al., ICLR 2023):** Even after permutation alignment, interpolated models suffer *variance collapse*: activations at the midpoint shrink dramatically. REPAIR fixes this by rescaling preactivations after alignment. 60-100% barrier reduction across architectures (74% for ResNet-50 on ImageNet).
        6. **The practical takeaway for model merging:** All of this is moot when models share a pretrained base. Shared pretraining puts neurons in corresponding positions from the start. No alignment needed. This is why every practical merging method assumes a common base model.
    6. **Layer-wise LMC (Adilova et al. 2023)**
        1. LMC doesn't hold uniformly across a network. Middle layers are the fragile ones; early and late layers tend to be more robust.
        2. This tells you *where* in the network merging is hardest, and motivates layer-wise merging strategies that treat different layers differently.
    7. **Linear Feature Connectivity (Zhou et al. 2023, LLFC)**
        1. When LMC holds in weight space, the *features* at every layer are also linearly connected. Interpolating weights produces interpolated feature maps, not garbage.
        2. This is a deeper result than LMC alone: it says the linearity extends from parameters all the way to representations.
    8. **Cross-Task Linearity (Zhou et al., ICML 2024)**
        1. **The key connection:** The pretraining-finetuning paradigm is structurally identical to Frankle's forking experiment. The "shared training" is pretraining; the "fork" is finetuning on different tasks. But prior results (LMC, LLFC) only studied models forked onto the *same* task with different SGD noise. Zhou et al. ask: what happens across *different* tasks?
        2. **The hierarchy of linear properties (show as a table):**
            - LMC (Frankle 2020): loss stays flat along interpolation path. Same task. Requires similar loss values.
            - LLFC (Zhou 2023): features at each layer are *proportional* to the interpolation. Same task. Stronger than LMC.
            - CTL (Zhou 2024): features at each layer are approximately *equal* to the interpolation. **Different tasks.** Does not require similar loss values.
        3. **How they measured it:** Take two models fine-tuned from the same base on different tasks (θ_i, θ_j). Interpolate their weights: θ_α = αθ_i + (1-α)θ_j. At each layer ℓ, compute the cosine similarity between:
            - The actual features of the interpolated model: f^(ℓ)(θ_α)
            - The linearly interpolated features: αf^(ℓ)(θ_i) + (1-α)f^(ℓ)(θ_j)
            If CTL holds, these two vectors point in almost exactly the same direction (cosine similarity close to 1).
        4. **What "linear map from parameter space to feature space" actually means — and why it's not trivial.**
            We normally think of a neural network as a function of its *input*: you feed in an image, you get features out. But you can also think of it as a function of its *weights*: fix the input, and ask how the features change as you change the weights.
            Concretely: take a single image of a cat.
            - Load model A (fine-tuned on task i). Feed the cat through. Record features at layer 5. Call this vector **a**.
            - Load model B (fine-tuned on task j). Feed the same cat through. Record features at layer 5. Call this vector **b**.
            - Now build model C with weights = 0.5·θ_A + 0.5·θ_B. Feed the same cat through. Record features at layer 5. Call this vector **c**.
            CTL says: **c** ≈ 0.5·**a** + 0.5·**b**. The features from the merged model are approximately the average of the features from each individual model. The network responds to weight changes linearly.
            **Addressing the naive objection:** You might think: "isn't this obvious? A single layer computes f(x) = σ(Wx + b). If you ignore the nonlinearity σ, then (αW_A + (1-α)W_B)x = αW_Ax + (1-α)W_Bx. That's just distributivity of matrix multiplication." And you'd be right — for a *single linear projection*, this is trivially true.
            **But CTL is a claim about the full composed network, not one layer.** Consider even a two-layer network. Layer 1 computes h = σ(W₁x), layer 2 computes σ(W₂h). When you interpolate the weights of both layers simultaneously, you get:
            σ((αW₂_A + (1-α)W₂_B) · σ((αW₁_A + (1-α)W₁_B) · x))
            Even if you could push the interpolation through the outer linear part, the inner σ blocks you. The input to layer 2 has already been nonlinearly transformed by the interpolated layer-1 weights. You can't factor this into separate contributions from model A and model B. The nonlinearities create cross-terms and interactions that should, in general, break linearity. For a deep composition of σ(W_ℓ · σ(W_{ℓ-1} · σ(... · x))), there is no mathematical reason interpolated weights should produce interpolated features.
            **So what is the actual insight?** The paper's finding is that pretrained neural networks, in the local neighborhood of the pretrained weights, *happen to behave* as if the mapping from weight space to feature space is approximately linear — despite being deeply nonlinear functions. Fine-tuning only moves weights slightly from the pretrained values. Most neurons stay in the same activation regime (firmly on or firmly off), so the nonlinearities don't change which "piece" of the piecewise-linear function you're on. The few neurons that switch regime don't matter much in aggregate.
            This is an *empirical finding about the geometry that pretraining creates*, not a mathematical tautology. It would not hold for randomly initialized networks, and it would not hold if you fine-tuned far enough from the pretrained initialization.
        5. **The finding:** CTL holds consistently across a wide range of settings. Tested on Rotated MNIST (MLP), Split CIFAR-100 (ResNet-18), ViTs on various image datasets, and T5 on text datasets.
        6. **Why LMC fails across tasks while CTL holds.**
            LMC and CTL measure different things. LMC asks: does the *loss* stay flat along the interpolation? CTL asks: do the *internal features* interpolate cleanly?
            When model A is fine-tuned on dogs-vs-cats and model B on cars-vs-trucks, LMC is evaluated on a specific task's loss. If you measure task A's loss along the interpolation, it starts low at model A and climbs toward model B (which knows nothing about dogs). Loss goes up. LMC fails.
            But CTL looks inside the network at the hidden representations. Those internal features are a clean, linear blend of what each model would produce. The representations are well-behaved. The loss barrier comes from the final layer: the classifier head is a blend of two heads designed for different label spaces, so it produces bad predictions for both tasks.
            **Why this matters for LLMs:** Language models share a single vocabulary head across all tasks. There is no task-specific classification layer to get confused. So the "clean features" that CTL guarantees translate more directly into useful merged outputs. This is part of why LLM merging works better in practice than vision-model merging.
        6. **Why it happens:** Two conditions: (a) the loss landscape around the pretrained model is flat (wide basin), and (b) fine-tuning produces small perturbations relative to the pretrained weights. Together these keep you in a regime where the network's nonlinearities are approximately linear.
        7. **What this means for merging:** If the network is a linear map from weights to features, then averaging weights = averaging features at each layer. Operations on parameters translate directly to operations on representations. This is the theoretical foundation for everything that follows.
        [Show the formula: f^(ℓ)(αθ_i + (1-α)θ_j) ≈ αf^(ℓ)(θ_i) + (1-α)f^(ℓ)(θ_j)]
        8. Transition: "The weight space around a pretrained model is approximately linear. Now let's talk about how to navigate it."
3. **Task Arithmetic (Ilharco et al., 2023)**
    1. **Define task vectors.** τ_A = θ_A - θ_pretrained. Fine-tuning moves the model from the pretrained weights in some direction. That direction *is* the task vector. It encodes everything the model learned about task A.
    [Animation: pretrained model as the origin. Fine-tuning on task A traces a path. The task vector is the arrow from origin to destination.]
    2. **Three operations that make task vectors powerful:**
        - **Addition:** θ_pretrained + τ_A + τ_B = a model that can do both tasks. This is what the hook scenario was doing, just stated precisely.
        - **Negation:** θ_pretrained - τ_A = a model that *forgets* task A. Useful for unlearning (e.g. removing toxic behavior).
        - **Scaling:** θ_pretrained + λτ_A = control the strength of task A's influence. λ > 1 amplifies, λ < 1 dampens.
    [Animation: vector addition in 2D/3D weight space. Two task vectors as arrows, their sum as the combined model.]
    3. **The connection to CTL.** §2 showed that the map from weights to features is approximately linear. So τ_A in weight space maps to a corresponding direction in feature space. Adding task vectors in weight space is approximately the same as adding the task-specific feature directions. This is the payoff of the theory section.
    4. **The reframing.** Task vectors shift the conversation from "averaging models" (which sounds lossy) to "combining learned capabilities" (which sounds compositional). Merging is no longer a blunt operation. It is vector arithmetic in a meaningful space.
4.  **The Interference Problem**
    1. **Addressing the obvious question:** "We just showed the weight space is approximately linear. Doesn't that mean addition just works?" CTL guarantees that merging in weight space produces *predictable* results in feature space. It does not guarantee that the predicted results are *desirable*. The map is linear, but the task vectors themselves can have overlapping, conflicting, or redundant components. Linearity tells you what the merged features will look like. It doesn't tell you those features are useful.
    2. **Where the interference comes from.** When you add τ_A + τ_B, some parameters were modified by both tasks. If task A pushed a parameter up and task B also pushed it up for a completely different reason, addition doubles that component. The features combine exactly as CTL predicts, but the combined feature vector is now over-saturated with mixed signal. If they pushed in opposite directions, the signals cancel. The problem is not the map breaking. The problem is that task vectors are not perfectly orthogonal: they share parameters, and shared parameters create cross-talk.
    [Animation: two task vectors with overlapping components. Where they agree, addition works. Where they conflict, you get destructive interference.]
    3. **Why it works well enough despite this.**
        1. Fine-tuning makes small perturbations relative to pretrained weights, so the nonlinearity is weak (local linearity, same reason CTL holds).
        2. Pretraining organizes weight space so different tasks tend to use different subsets of parameters. Task vectors are *approximately* orthogonal, so cross-terms are small.
    4. **Weight Disentanglement (Ortiz-Jimenez et al., NeurIPS 2023 Oral).** This paper formalizes the intuition above. "Weight disentanglement" means distinct directions in weight space govern separate, localized regions in function space for different tasks. When this holds, task vectors don't interfere because they operate in different parts of the network's behavior. The paper shows that pretraining naturally produces weight disentanglement, and that linearizing the model (computing task vectors in the tangent space rather than naively) amplifies disentanglement and significantly improves task arithmetic performance.
    5. **Some common failure modes that merging methods target:**
        1. **Sign conflicts:** The same parameter is pushed positive by one task and negative by another. The sum cancels out, losing both signals.
        2. **Redundant updates:** Many parameters get tiny, noisy updates that don't contribute meaningfully but add up to interference.
        3. **Magnitude imbalance:** One task's vector dominates another's, drowning out the weaker task.
        4. **Subspace misalignment:** Task vectors live in overlapping subspaces, so addition mixes unrelated learned structure.
    6. Each of these failure modes motivates a specific family of merging methods. That's what we cover next.
5. **Chronology**
    
    ![image.png](Outline/image.png)
    
    "There are dozens of methods. Here's how they arrived over time. Let me walk you through the ones that matter."

6. **Model Merging Methods**
    1. **Stochastic Weight Averaging (SWA)** (Izmailov et al., 2018)
        - The earliest practical demonstration that averaging weights produces better models.
        - Averages checkpoints along a single training trajectory (with cyclical or high-constant learning rate). The averaged model sits in a wider, flatter basin than any individual checkpoint.
        - Not "merging" in the modern sense (it's a training-time technique on one model), but it established the key insight: the average of good weights is often better than any individual set of weights.
        - Sets the conceptual foundation for everything that follows.
    2. **Model Soups** (Wortsman et al., 2022, ~800 citations)
        - The paper that proved post-hoc averaging of independently fine-tuned models works.
        - Take a pretrained model, fine-tune it N times with different hyperparameters (learning rate, augmentation, etc.), then just average the resulting weights.
        - "Greedy soup": add models one at a time, keeping each addition only if it improves held-out accuracy. Simple but effective.
        - Achieved state-of-the-art on ImageNet and several distribution shift benchmarks without any additional training.
        - The direct ancestor of modern model merging: proved that the loss basin around a pretrained model is wide enough for averaging to work across diverse fine-tuning runs.
    3. **Fisher Merging** (Matena & Raffel, 2022, ~500 citations)
        - The first *principled* approach to merging. Instead of treating all parameters equally, weight each parameter by its Fisher information — a measure of how important that parameter is for each task.
        - Formula: θ_merged = Σ F_i · θ_i / Σ F_i, where F_i is the diagonal Fisher information matrix for task i.
        - Key conceptual contribution: **not all parameters matter equally during merging**. This idea reappears in every subsequent method.
        - Limitation: computing the full Fisher is expensive; in practice uses a diagonal approximation. Outperformed by simpler methods (TIES, DARE) on most benchmarks, but the idea was foundational.
    4. **RegMean** (Jin et al., 2023)
        - Closed-form optimal merging for linear layers. Instead of averaging weights, computes the least-squares optimal merged weight matrix using input activation statistics.
        - For a linear layer y = Wx, the optimal merged W minimizes reconstruction error across all tasks' data distributions. Has a clean closed-form solution.
        - Shows that for linear layers, you can compute the mathematically *best possible* merge. Fast, no hyperparameters.
        - Practically effective and underappreciated. Works especially well for transformer attention/FFN layers which are linear projections.
    5. **Task Arithmetic** (Ilharco et al., 2023) — covered conceptually in §3, but also the baseline method. θ_pretrained + Σ λ_i · τ_i. The simplest task-vector method and the one all others are compared against.
    6. **TIES-Merging** (Yadav et al., NeurIPS 2023, ~400 citations)
        - Problem: sign conflicts and redundant parameters (failure modes 1 and 2 from §4).
        - Fix: three-step pipeline: (1) **Trim** low-magnitude deltas, (2) **Elect** a consensus sign for each parameter across models, (3) **Merge** only the agreeing parameters.
        - Defined the vocabulary the field still uses. A must-know method.
        [Animation: show a parameter vector with conflicting signs across tasks, then the trim-elect-merge pipeline cleaning it up]
    7. **DARE** (Yu et al., ICML 2024, ~300 citations, "Super Mario" paper)
        - Problem: redundant updates (failure mode 2). Most delta parameters are near-zero noise.
        - Fix: randomly drop 90-99% of delta parameters and rescale survivors by 1/(1-p). The surviving parameters carry the signal.
        - Counterintuitive: throwing away almost everything works better than keeping it all.
        - DARE-TIES (combining both) was arguably the most popular practical method in 2024-25. Produced #1 model on the Open LLM Leaderboard.
        - Mention DELLA (2024) as a refinement: magnitude-based dropout instead of uniform random.

7. **Frontier Methods**
    1. **Subspace Boosting** (Skorobogat et al., 2025)
        - Identifies **rank collapse** as a fundamental, provable limitation of Task Arithmetic-based methods.
        - As you merge more models, common information grows at O(N) while task-specific information decays at O(1/sqrt(N)). The unique knowledge gets drowned out. This is a theorem, not just an empirical observation.
        - The fix: decompose merged task vectors via SVD, boost the suppressed singular values. One hyperparameter. >10% improvement across vision and language.
        - Also introduces HO-GSVD for interpretable merging: comparing task similarity via shared subspaces, selecting which models to merge.
        [Animation: singular value distribution. As you add more models, the tail collapses. Subspace Boosting lifts it back up.]
    2. **AdaMerging** (Yang et al., NeurIPS 2023)
        - Learns merging coefficients automatically rather than hand-tuning them.
        - Two variants: **Task-wise** (one coefficient per task) and **Layer-wise** (one coefficient per task *per layer*). Layer-wise is much more expressive and consistently better.
        - Uses entropy minimization on unlabeled test data to find optimal coefficients — no labeled data needed.
        - Bridges hand-crafted merging and learned merging. Conceptually the "what if we just optimize the recipe?" approach.
    3. **Evolutionary Model Merge** (Sakana AI / Akiba et al., Nature Machine Intelligence, 2025)
        - Paradigm shift: instead of hand-tuning merge recipes, use evolutionary algorithms to search over both mixing coefficients and layer permutations.
        - Produced a 7B Japanese math model that beat some 70B models.
        - The move from "how to merge" to "automatically discover what to merge."
    4. **WARM — Weight Averaged Reward Models** (Rame et al., ICML 2024, DeepMind)
        - Applies model merging to a different problem: alignment via RLHF.
        - Instead of using a single reward model, train multiple RMs independently and average their weights. The merged RM is more robust than any individual one.
        - Key result: averaged reward models resist **reward hacking** better than single or ensemble RMs. 79.4% win rate over single-RM baseline.
        - Why it matters: shows merging is not just a model compression trick — it's a tool for alignment and safety. Merging as regularization.
    5. **Training for Mergeability — Effective Noise Scale** (Zhang et al., 2025)
        - A single quantity, the effective noise scale (function of learning rate, batch size, weight decay, augmentation), controls merging compatibility.
        - Non-monotonic relationship: too little noise = incompatible models, too much = unstable training. There is a sweet spot.
        - Practical levers: larger learning rate, stronger weight decay, smaller batch size all make models more mergeable.
        - Flips the question from "how to merge better" to "how to *train* for merging." A shift in perspective.
    6. **Frankenmerges / Passthrough Merging** (community-driven, 2023-24)
        - Not a paper — a community technique. Stack layers from different models: e.g., layers 0-16 from model A, layers 17-32 from model B.
        - No theoretical justification, but produced several top Open LLM Leaderboard models.
        - Worth mentioning as evidence that the community was ahead of academia in exploring the method space.
        - Can also produce models with *more* layers than either parent (e.g., combining two 32-layer models into a 48-layer model by selecting subsets).

8. **What Actually Works at Scale?**
    1. Reference: https://www.emergentmind.com/topics/model-merging-benchmark
    2. This [preprint](https://arxiv.org/html/2511.21437v1) claims that for larger LLMs, only Task Arithmetic shows promising results. The other methods had large deviations from the base model, suggesting they were either too aggressive or badly tuned.
        
        ![image.png](Outline/image%201.png)
        
    3. Acknowledge that most studies/methods were done on SLMs or vision tasks.
    4. MergeBench (NeurIPS 2025) as a step toward standardized evaluation.

9. **Practical Guidance**
    1. **What matters most at scale** (Yadav et al., "What Matters for Model Merging at Scale?", 2024):
        - **Base model quality is the single biggest factor.** Merging works dramatically better when experts are created from strong base models with good zero-shot performance. Start from the best base you can get.
        - **Larger models merge more easily.** At 64B parameters, merging is consistently effective; at 1B, it's fragile. Larger models can also absorb more experts before degrading.
        - **Merging improves generalization.** Merged models generalize better to held-out tasks than individual experts, and at scale (8 experts, large models), even outperform multitask-trained models.
        - **Method choice matters less at scale.** Averaging, Task Arithmetic, DARE-TIES, and TIES all converge to similar performance at large model sizes. At small scales, method choice matters more.
    2. **Key parameters and their documented ranges** (from MergeKit documentation and Labonne's practitioner guides):
        - **SLERP `t`:** interpolation factor between 0 and 1. Can be set per-layer or per-module type (e.g., different `t` for self-attention vs MLP layers). Community recipes often use gradient values like `[0, 0.5, 0.3, 0.7, 1]` across layers.
        - **TIES `density`:** fraction of delta parameters to keep. Documented range: 0.2 (aggressive trim) → 0.5 (default/general purpose) → 0.7 (preserve distinct changes) → 1.0 (sign election only, no trimming).
        - **TIES `weight`:** relative importance of each model. Use `normalize: true` to auto-normalize weights.
        - **DARE `density`:** fraction of delta parameters to *keep* (not drop). The original paper recommends <0.5, but community practice finds 0.5-0.7 often works better (see MergeKit GitHub issue #26). Use `int8_mask: true` for efficiency.
        - **DARE weights:** should sum to between 0.9 and 1.1 when using `dare_ties`.
    3. **Prerequisites for merging:**
        - Models must share the same architecture (non-negotiable).
        - Models should be fine-tuned from the same base (the theoretical foundation from §2).
        - No access to training data needed — only weights.
    4. **Practical tips from the community** (Labonne, MergeKit docs):
        - SLERP is limited to 2 models but can be applied hierarchically (merge A+B, then merge result with C).
        - MergeKit supports per-layer slice configurations: you can merge different layer ranges from different models (frankenmerging).
        - The `--lazy-unpickle` and `--out-shard-size` flags allow merging on CPUs with as little as 8GB RAM.
        - Evaluate on multiple benchmarks, not just one. The Open LLM Leaderboard is contaminated — merged models inherit contamination from their parents.
    5. **Tooling**: MergeKit (Arcee, open-source, supports SLERP/TIES/DARE/DELLA/evolutionary/passthrough/MoE), FusionBench (systematic benchmarking), LazyMergeKit (Labonne's one-click Colab notebook).

10. **The Bigger Picture: Why This Matters**
    1. The Hugging Face explosion: hundreds of thousands of fine-tuned models, most derived from the same bases (Llama, Qwen, Mistral). Show HF screenshot. A massive supply of specialist models sitting in the same loss basin.
    2. Model merging was the first technique the open-source community adopted at scale. Merged models dominated the Open LLM Leaderboard throughout 2023-2024.
    3. Frontier labs use it in their workflows:
        1. **Cohere** published on recycling suboptimal training checkpoints via weight merging in their ~100B model development pipeline ("If You Can't Use Them, Recycle Them", 2024)
        2. **ByteDance** validated pre-training model averaging at 100B+ scale across dense and MoE architectures (PMA, 2025)
        3. **Meta** developed Branch-Train-MiX as a scalable training paradigm built on merging
        4. **Deepseek** uses merging during distillation by distilling multiple specialist models from their teacher model and then merging the distilled models.
    4. Practical use cases:
        1. **Combining capabilities cheaply**: merge specialists in code, chat, and instruction following with no GPU needed
        2. **Decentralized training**: different teams fine-tune independently, then combine results without sharing data
        3. **Continual learning without catastrophic forgetting**: merge in new knowledge without destroying old capabilities
        4. **Diverse alignment**: Sakana Labs showed you can merge models fine-tuned on different cultural or value preferences

11. **Open Frontiers**
    1. **Scaling laws for merging** (2025): power-law relationships between model size, number of experts, and merged performance. Diminishing returns are quantifiable. The "Chinchilla moment" for merging: tells you when to stop adding experts and scale the base model instead.
    2. **PMA** (ByteDance, 2025): merging during pre-training, not post-hoc. Changes the scale of what's possible. Merging as a first-class training primitive.
    3. **MoErging**: the bridge between static merging and MoE. "Merge where you can, route where you must." Being covered in a separate talk.
    4. The field is moving from "a post-hoc trick" to "core infrastructure for model development."